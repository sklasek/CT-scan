---
title: "01_sequence_processing"
author: "Scott Klasek"
date: "6/6/2020"
output: github_document
---

In this project, we investigate how X-ray CT scanning changes microbial community structure in tidal estuary sediments using 16S rRNA sequence data.  

We begin by determining ASVs from raw illumina sequence reads in .fastq format using DADA2, which gives a sequence table. Then define taxonomy and write metadata. Taxonomy and metadata tables are combined with the sequence table to make a phyloseq object.  

## 0) Load the required libraries into the RStudio environment:  

```{r libraries}
library(dada2)
library(tidyr)
library(plyr)
library(dplyr)
library(ggplot2)
library(phyloseq)
library(decontam)
library(DECIPHER)
library(phangorn)
library(here)
here()
```

## 1) Define paths and input files & inspect read quality

```{r setup}
# define paths and input files

fqpath <- "/Users/scottklasek/Desktop/collabs/CT_scan_communities/raw_fastq" # define path to fastq files
fnFs <- sort(list.files(fqpath, pattern="_R1.fastq", full.names = TRUE)) # get names of F reads
fnRs <- sort(list.files(fqpath, pattern="_R2.fastq", full.names = TRUE)) # get names of R reads
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1) # get names of samples from F reads
filtFs <- file.path(fqpath, "filtered", paste0(sample.names, "_F_filt.fastq")) # define path and suffixes for filtered F reads
filtRs <- file.path(fqpath, "filtered", paste0(sample.names, "_R_filt.fastq")) # define path and suffixes for filtered R reads
names(filtFs) <- sample.names # assign F and R names
names(filtRs) <- sample.names

# inspect F and R read quality (reads are 251 bp long)

plotQualityProfile(fnFs[1:2]) # F reads from the first two samples looks good, trim at 240
plotQualityProfile(fnRs[1:2]) # R read from the first two samples looks less good, trim at 160
```

## 2) Use DADA2 to filter sequences, learn errors, and infer ASVs. Merge them from F and R sequences, trim merged sequences, remove chimeras, and track reads through the workflow:  

```{r asvs}
# Filter sequences. add trimLeft=c(17,18) to trim primers if necessary
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)

# Calculate errors
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)

# Obtain ASVs
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

# Merge F and R ASV sequences
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
seqtab <- makeSequenceTable(mergers)
table(nchar(getSequences(seqtab)))

# Trim sequences
seqtab.trim <- seqtab[,nchar(colnames(seqtab)) %in% 251:256]

# Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab.trim, method="consensus", multithread=TRUE, verbose=TRUE)

# Track sequences through the workflow
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
tdf <- as.data.frame(track)
tdf$filtered_out <- tdf$input-tdf$filtered
tdf$noise_removed <- tdf$filtered-with(tdf, pmin(denoisedF, denoisedR))
tdf$unmerged <- (tdf$filtered-tdf$noise_removed)-tdf$merged
tdf$chimeras <- tdf$merged-tdf$nonchim
tdf <- data.frame(sample = row.names(tdf), tdf)
tdfs <- tdf[,c(1,7,8,9,10,11)]
tdfl <- gather(tdfs, step, reads, nonchim:chimeras, factor_key=FALSE)
tdfl$step <- factor(tdfl$step, levels = c("filtered_out","noise_removed","unmerged","chimeras", "nonchim"))
track.reads <- ggplot(tdfl,aes(sample,reads,fill=step))
track.reads.plot <- track.reads+
  geom_bar(stat="identity")+
  scale_y_continuous(breaks = c(1000,5000,50000,100000,150000,200000,250000))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
track.reads.plot
```

## 3) Assign taxonomy

```{r taxonomy}
taxa <- assignTaxonomy(seqtab.nochim, "~/Desktop/flux_resubmission/silva_nr_v132_train_set.fa", multithread=TRUE)
# taxa <- addSpecies(taxa, "~/Desktop/flux_resubmission/silva_species_assignment_v132.fa") you can assign species if you have more memory than this computer does:
# Error: vector memory exhausted (limit reached?)
taxa.print <- taxa
rownames(taxa.print) <- NULL
head(taxa.print)
```

## 4) Obtain metadata from sample names (directly and indirectly):  

```{r metadata}
# create data frame
metadata <- data.frame("sample_name"=sample.names,
                       "core"=substring(sample.names, 1,1),
                       "depth_cm"=as.numeric(substring(sample.names, 3,4)),
                       "date"=as.numeric(substring(sample.names, 6,7)))

# assign scanned/unscanned cores
metadata$scan <- ifelse(metadata$core == "2", "Unscanned",
                        ifelse(metadata$core == "3", "Scanned",
                        ifelse(metadata$core == "4", "Unscanned",
                        ifelse(metadata$core == "5", "Scanned",
                        ifelse(metadata$core == "B", NA, NA)))))

# assign north/south site
metadata$site <- ifelse(metadata$core == "2", "North",
                        ifelse(metadata$core == "3", "North",
                        ifelse(metadata$core == "4", "South",
                        ifelse(metadata$core == "5", "South",
                        ifelse(metadata$core == "B", NA, NA)))))
# make rownames sample names
rownames(metadata) <- sample.names

# write the storage time as a function of date
metadata$storage_days <- metadata$date-5 # cores collected on July 5

# define sediment type (tsunami deposit vs non-deposit)
metadata$sedtype <- ifelse(
  ((metadata$site=="North") &
    (metadata$depth_cm < 63) &
    (metadata$depth_cm > 59) |
    (metadata$site=="South") &
    (metadata$depth_cm < 71) &
    (metadata$depth_cm > 68)),
"Tsunami Deposit",
"Estuary Sediment"
)
```

## 5) Make a phylogenetic tree for Unifrac calculations:  

```{r phylogenetic tree}
seqs <- getSequences(seqtab.nochim)
names(seqs) <- seqs
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA) # this took ~50 minutes
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align) # started 3:05 pm, finished around an hour later? 
treeNJ <- NJ(dm) # Note, tip order != sequence order ## this takes at least an hour... started 6:01 pm
fit <- pml(treeNJ, data=phang.align)
fitGTR <- update(fit, k=4, inv=0.2)
```

## 6) Combine all these into a phyloseq object and make a DNA stringset:  

```{r stringset and phyloseq object}
ps1 <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(metadata), 
               phy_tree(fitGTR$tree),
               tax_table(taxa))
dna <- Biostrings::DNAStringSet(taxa_names(ps1))
names(dna) <- taxa_names(ps1)
ps1 <- merge_phyloseq(ps1, dna)
taxa_names(ps1) <- paste0("ASV", seq(ntaxa(ps1)))
saveRDS(ps1, file = "ps1")
ps1
```

## 7) Remove unwanted taxa from the phyloseq object

```{r}
# make a table showing numbers of ASVs and their reads from these groups:
ASV_classifications <- c("Bacteria","Archaea","Eukaryota", "Unclassified", "Chloroplast","Mitochondria")
num_ASVs <- c(sum(tax_table(ps1)[,1]=="Bacteria", na.rm = TRUE),
               sum(tax_table(ps1)[,1]=="Archaea", na.rm=TRUE),
               sum(tax_table(ps1)[,1]=="Eukaryota", na.rm=TRUE),
               sum(is.na(tax_table(ps1)[,1])),
               sum(tax_table(ps1)[,4]=="Chloroplast", na.rm=TRUE), 
               sum(tax_table(ps1)[,5]=="Mitochondria", na.rm=TRUE))
num_ASVcounts <- c(sum(otu_table(ps1)[,which(tax_table(ps1)[,1]=="Bacteria")], na.rm = TRUE),
              sum(otu_table(ps1)[,which(tax_table(ps1)[,1]=="Archaea")], na.rm = TRUE),
              sum(otu_table(ps1)[,which(tax_table(ps1)[,1]=="Eukaryota")], na.rm = TRUE), 
              sum(is.na(tax_table(ps1)[,1])),
              sum(otu_table(ps1)[,which(tax_table(ps1)[,4]=="Chloroplast")], na.rm = TRUE), 
              sum(otu_table(ps1)[,which(tax_table(ps1)[,5]=="Mitochondria")], na.rm = TRUE))
asv.table <- cbind.data.frame(ASV_classifications, num_ASVs, num_ASVcounts)
asv.table

ps2 <- subset_taxa(ps1, (Kingdom!="Eukaryota")) # unknown and eukaryote ASVs removed
ps2 <- subset_taxa(ps2, (Order!="Chloroplast") | is.na(Order)) # chloroplasts removed
ps2 <- subset_taxa(ps2, (Family!="Mitochondria") | is.na(Family)) # mitochondria removed
ps2 # 98 Unknown, Eukaryote, Mitochondrial, and Chloroplast taxa removed 
```

## 8) Use decontam to identify and remove contaminant

```{r}
sample_data(ps2)$is.neg <- sample_data(ps2)$core == "B" # Identify the extraction blank as $is.neg
ps2.decontam.prev <- isContaminant(ps2, method="prevalence", neg="is.neg") # run decontam, prevalence method (don't have quantitative data for frequency)
decontam.prev.table <- table(ps2.decontam.prev$contaminant)
decontam.prev.table # One contaminant identified

ps2.decontam.prev05 <- isContaminant(ps2, method="prevalence", neg="is.neg", threshold = 0.5) # run decontam, prevalence method with a higher threshold of 0.5
decontam.prev05.table <- table(ps2.decontam.prev05$contaminant)
decontam.prev05.table # Still only one contaminant identified

ps2.decontam.prev.mx <- subset(tax_table(ps2)[which(ps2.decontam.prev$contaminant),]) 
ps2.decontam.prev.mx # it's a methanolobus

contam <- rownames(ps2.decontam.prev.mx) # the contaminant ASV
allnames <- rownames(tax_table(ps2)) # the taxa names
ps2.uncontam <- allnames[!allnames %in% contam] # gives us the noncontaminant ASVs
length(ps2.uncontam) # 7123 noncontaminant ASVs
ps3 <- prune_taxa(ps2.uncontam, ps2) # remove the contaminant
ps3
```

## 9) Identify and prune low-abundance samples (and blank):

```{r}
df.ct <- as.data.frame(sample_data(ps3)) # make a dataframe from the sample data
df.ct$LibrarySize <- sample_sums(ps3) # obtain library size
df.ct <- df.ct[order(df.ct$LibrarySize),] # order by library size
df.ct$Index <- seq(nrow(df.ct)) # index by library size
ct.sample.depth <- ggplot(data=df.ct, aes(x=Index, y=LibrarySize, color=core)) + 
  geom_point() + 
  scale_y_continuous(breaks = c(2000,5000,10000,25000))
  ggtitle("After decontamination") # plot
df.ct[1:12,] # ten samples have fewer reads than the blank and should obviously be discarded. the last sample here has 4622 reads
ps4 <- prune_samples(sample_sums(ps3)>=4622, ps3)
ps4
saveRDS(ps4, file = "ps4")
```

Now we're done. Move on to 02_sequence_analysis.Rmd


